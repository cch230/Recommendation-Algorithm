# *Recommendation-Algorithm* 
![image](img/algorithm_img.png)

## View English Introduction
 - [Click to read English introduction.](#_Index)

## ðŸ™ í›„ì› ì•ˆë‚´ (Support & Sponsor)

ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´, ê°œë°œ ì§€ì†ê³¼ ìœ ì§€ë³´ìˆ˜ë¥¼ ìœ„í•´ í›„ì›ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤!  
ì—¬ëŸ¬ë¶„ì˜ ìž‘ì€ ì‘ì›ì´ ì˜¤í”ˆì†ŒìŠ¤ ë°œì „ì— í° íž˜ì´ ë©ë‹ˆë‹¤.

- [GitHub Sponsorsë¡œ í›„ì›í•˜ê¸°](https://github.com/sponsors/ì—¬ëŸ¬ë¶„ì˜_ì•„ì´ë””)
- ë˜ëŠ” ì»¤í”¼ í•œ ìž”ì„ ë³´ë‚´ì£¼ì„¸ìš”! â˜•

If you find this project useful, please consider supporting it!  
Your sponsorship helps keep this project alive and motivates further development.

- [Sponsor via GitHub Sponsors](https://github.com/sponsors/your_id)
- Or just buy me a coffee! â˜•

ê°ì‚¬í•©ë‹ˆë‹¤! Thank you!

---
## Index 
- [Directory](#Directory)
- [DataSet](#DataSet)
- [Annoy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ë²¡í„° ìœ ì‚¬ë„ ìµœì í™”](#annoy-ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼-í™œìš©í•œ-ë²¡í„°-ìœ ì‚¬ë„-ìµœì í™”)
- [t-SNEë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„° ì‹œê°í™”](#t-SNEë¥¼-í™œìš©í•œ-ë²¡í„°-ë°ì´í„°-ì‹œê°í™”)
- [í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT ëª¨ë¸ì„ í™œìš©í•œ ë‹¨ì–´ í•„í„°ë§](#í•œêµ­ì–´-í…ìŠ¤íŠ¸-ë¶„ë¥˜ë¥¼-ìœ„í•œ-bert-ëª¨ë¸ì„-í™œìš©í•œ-ë‹¨ì–´-í•„í„°ë§)
- [konlpyì™€ googletransë¥¼ í™œìš©í•œ ì¹´í…Œê³ ë¦¬ í˜•íƒœì†Œ ë¶„ì„ ë° ë²ˆì—­](#konlpyì™€-googletransë¥¼-í™œìš©í•œ-ì¹´í…Œê³ ë¦¬-í˜•íƒœì†Œ-ë¶„ì„-ë°-ë²ˆì—­)
- [Word2Vec í•œêµ­ì–´ ë‹¨ì–´ ìž„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•](#word2vec-í•œêµ­ì–´-ë‹¨ì–´-ìž„ë² ë”©-ë°ì´í„°ë² ì´ìŠ¤-êµ¬ì¶•)
- [ì˜ë¯¸ë¡ ì  ë‹¨ì–´ìœ ì‚¬ë„ë¥¼ í™œìš©í•œ ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ ì¶”ì²œ](#ì˜ë¯¸ë¡ ì -ë‹¨ì–´ìœ ì‚¬ë„ë¥¼-í™œìš©í•œ-ì¹´í…Œê³ ë¦¬í‚¤ì›Œë“œ-ì¶”ì²œ)
- [ë¼ì´ì„ ìŠ¤](#ë¼ì´ì„ ìŠ¤)


---
## Directory
- [`USER_CTGY`](USER_CTGY): ì‚¬ìš©ìž ê¸°ë°˜ í˜‘ì—…í•„í„°ë§ì„ í†µí•œ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜(User-based CF)
- [`USER_MODL`](USER_MODL): ì½˜í…ì¸  ê¸°ë°˜ í˜‘ì—…í•„í„°ë§ì„ í†µí•œ ë¬˜ë“ˆë³„ ê¸°ëŠ¥ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜(Item-based CF)
- [`SMLR_RECO`](SMLR_RECO): í˜•íƒœì†Œ ë¶„ì„, íƒœê¹… ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ì˜ë¯¸ë¡ ì  í‚¤ì›Œë“œ, ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜
  1. [`ByCTGY`](SMLR_RECO/ByCTGY): ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì—°ê´€ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜
  2. [`ByKYWD`](SMLR_RECO/ByKYWD): í‚¤ì›Œë“œ ê¸°ë°˜ ì—°ê´€ ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜
  

---
## DataSet
1. ìœ ì €ì˜ ëª…ì‹œì  ë°ì´í„°ì™€ ì¹´í…Œê³ ë¦¬/ëª¨ë“ˆ ë³„ í–‰ë™ ê¸°ë¡ì„ ë¶„ì„í•œ ì•”ì‹œì  í”¼ë“œë°±ì„ í™œìš©í•œ ì¹´í…Œê³ ë¦¬/ëª¨ë“ˆ ë²¡í„° ë°ì´í„°
   - ë™ì  ë²¡í„° ê°€ì¤‘ì¹˜: ì‚¬ìš©í•œ í¬ì¸íŠ¸, ë”ë³´ê¸° ìš”ì²­, ê²€ìƒ‰, 30ì´ˆ ì´ìƒ ì²´ë¥˜, ì €ìž¥/ê°±ì‹  í™œì„±í™”, ì¢‹ì•„ìš” / ëŒ“ê¸€
   - ì •ì  ë²¡í„° ê°€ì¤‘ì¹˜: ëª¨ë“ˆ, ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ, ì—°ê°„ í‚¤ì›Œë“œ
2. spellcheck-koì—ì„œ ì œê³µí•˜ëŠ” [í•œêµ­ì–´ê¸°ì´ˆì‚¬ì „](https://krdict.korean.go.kr/), [í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „](https://stdict.korean.go.kr/), [ìš°ë¦¬ë§ìƒ˜](https://opendict.korean.go.kr/) ê¸°ë°˜  [í•œêµ­ì–´ ë§žì¶¤ë²• ì‚¬ì „](https://github.com/spellcheck-ko/hunspell-dict-ko/releases/download/0.7.92/ko-aff-dic-0.7.92.zip)
3. Facebookì—ì„œ ì œê³µí•˜ëŠ” FastTextì˜ 300ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ë‹¨ì–´ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë°˜ì˜í•œ í•œêµ­ì–´ Word2Vec ëª¨ë¸ [í•œêµ­ì–´ ë‹¨ì–´ ë²¡í„°](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ko.300.vec.gz)
4. ë„¤ì´ë²„ ì¹´í…Œê³ ë¦¬ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ë‚˜ëˆˆ ë„¤ì´ë²„ ì¹´í…Œê³ ë¦¬ ë§ ë­‰ì¹˜


---
## Annoy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ë²¡í„° ìœ ì‚¬ë„ ìµœì í™”
[`Annoy`](https://github.com/spotify/annoy)ì™€ [`Bayesian Optimization`](https://github.com/bayesian-optimization/BayesianOptimization) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ìœ ì‚¬ë„ë¥¼ ìµœì í™” í•©ë‹ˆë‹¤.  

### ì‚¬ìš©ëœ ì£¼ìš” ê¸°ìˆ  ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Annoy ë¼ì´ë¸ŒëŸ¬ë¦¬:** ë²¡í„° ìœ ì‚¬ë„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê³  ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Bayesian Optimization:** ëª©ì  í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜
- **pandas:** ë°ì´í„° ì¡°ìž‘ ë° ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy:** ë‹¤ì°¨ì› ë°°ì—´ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **matplotlib:** ë°ì´í„° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬

### ì‚¬ìš©ë²•
1. **ì˜ì¡´ì„± ì„¤ì¹˜:**
   ```bash
   pip install annoy pandas numpy scikit-learn bayesian-optimization matplotlib

2. **ì½”ë“œ ì‹¤í–‰:**
   ```bash
   python *_optimizeAnnModel.py
ìœ„ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ Annoy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ë²¡í„° ìœ ì‚¬ë„ ìµœì í™”ë¥¼ ìˆ˜í–‰

### íŒŒì´ì¬ ì½”ë“œ íŒŒì¼ (*_similarity_optimization.py)ì— ëŒ€í•œ ì„¤ëª…
- `evaluate_n_trees(n_trees)`: Annoy ì¸ë±ìŠ¤ì˜ ì •í™•ë„ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ë¡œ, ì£¼ì–´ì§„ íŠ¸ë¦¬ ìˆ˜ì— ëŒ€í•´ ë²¡í„° ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  ìµœê·¼ì ‘ ì´ì›ƒë“¤ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ë°˜í™˜
- `BayesianOptimization`: íŠ¸ë¦¬ ìˆ˜(n_trees)ë¥¼ ì¡°ì •í•˜ì—¬ ëª©ì  í•¨ìˆ˜(evaluate_n_trees)ë¥¼ ìµœì ì˜ í‰ê·  ê±°ë¦¬ë¥¼ íƒìƒ‰í•˜ë©° íŠ¸ë¦¬ ìˆ˜ ìµœì í™”
  

---  
## t-SNEë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„° ì‹œê°í™” 
scikit-learnì˜ [`t-SNE`](https://github.com/scikit-learn/scikit-learn/tree/main) ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ ë²¡í„° ë°ì´í„°ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤.

### ì‚¬ìš©ëœ ì£¼ìš” ê¸°ìˆ  ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **t-SNE:** ê³ ì°¨ì› ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì €ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ì‹œê°í™”í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜
- **matplotlib:** ë°ì´í„° ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
- **scikit-learn:** ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ êµ¬í˜„ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **pandas:** ë°ì´í„° ì¡°ìž‘ ë° ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy:** ë‹¤ì°¨ì› ë°°ì—´ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

### ì‚¬ìš©ë²•

1. **ì˜ì¡´ì„± ì„¤ì¹˜:**
   ```bash
   pip install scikit-learn matplotlib pandas numpy
2. **ì½”ë“œ ì‹¤í–‰:**
   ```bash 
   python visualize_vectors.py
TSNE_3D.png ë° TSNE_2D.png ì´ë¯¸ì§€ íŒŒì¼ë¡œ 2D ë° 3D t-SNE ê²°ê³¼ê°€ ìƒì„±
  

--- 
## í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT ëª¨ë¸ì„ í™œìš©í•œ ë‹¨ì–´ í•„í„°ë§
í…ìŠ¤íŠ¸ì—ì„œ ê¹¨ë—í•œ ë‹¨ì–´ë¥¼ í•„í„°ë§í•˜ê¸° ìœ„í•´ ìžì—°ì–´ ì²˜ë¦¬(NLP) ìž‘ì—…ì— ì‚¬ìš©ë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì¸ BERT ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ì‚¬ìš©ëœ ì£¼ìš” ê¸°ìˆ  ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **BERT:** ì–‘ë°©í–¥ íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸, Smilegate-aiì—ì„œ ì œê³µí•˜ëŠ” [`kor_unsmile`](https://github.com/smilegate-ai/korean_unsmile_dataset) ëª¨ë¸ì„ í™œìš©
- **Hugging Face Transformers:** ë‹¤êµ­ì–´ë¡œ ëœ ì—¬ëŸ¬ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬, ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰

### ì‚¬ìš©ë²•

1. **ì˜ì¡´ì„± ì„¤ì¹˜:**
   ```bash
   pip install transformers tqdm

2. **ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ:**
    ```python
    from transformers import BertForSequenceClassification, AutoTokenizer
    
    model_name = 'smilegate-ai/kor_unsmile'
    model = BertForSequenceClassification.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
3. **ë‹¨ì–´ í•„í„°ë§ ìˆ˜í–‰:**
    ```bash
   python filter_words.py
spellcheck-koì—ì„œ ì œê³µí•˜ëŠ” í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ë¶„ë¥˜í•˜ê³  ê¹¨ë—í•œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ì—¬ ê²°ê³¼ëŠ” data/ko_filtered.txt íŒŒì¼ì— ì €ìž¥

### íŒŒì´ì¬ ì½”ë“œ íŒŒì¼ (filter_words.py)ì— ëŒ€í•œ ì„¤ëª…
 - `get_predicated_label(output_labels, min_score)`: BERT ëª¨ë¸ì˜ ì¶œë ¥ ë ˆì´ë¸”ì—ì„œ ì§€ì •ëœ ìµœì†Œ ì ìˆ˜ ì´ìƒì¸ ë ˆì´ë¸”ë§Œì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

- `TextClassificationPipeline`: í…ìŠ¤íŠ¸ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì„¤ì •. í…ìŠ¤íŠ¸ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„ BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜
  

---
## konlpyì™€ googletransë¥¼ í™œìš©í•œ ì¹´í…Œê³ ë¦¬ í˜•íƒœì†Œ ë¶„ì„ ë° ë²ˆì—­ 
[`KoNLPy`](https://github.com/konlpy/konlpy)ì˜ ì—¬ëŸ¬ íƒœê¹… ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ë¡œ ì¶”ì¶œí•˜ê³ , [`googletrans`](https://github.com/ssut/py-googletrans)ë¥¼ í™œìš©í•˜ì—¬ ì¶”ì¶œëœ ë‹¨ì–´ë“¤ì„ ë²ˆì—­ í•œí›„ ì •ê·œí™”ë¥¼ ê±°ì³ ìƒˆë¡œìš´ ìœ ì‚¬ ë‹¨ì–´ë“¤ì„ í™•ë³´í•©ë‹ˆë‹¤.

### ì‚¬ìš©ëœ ê¸°ìˆ  ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **konlpy**: í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬, Okt, Hannanum, Kkma, Komoranì„ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰
- **googletrans**: Google Translate APIë¥¼ í™œìš©í•˜ì—¬ ë‹¨ì–´ë¥¼ ë²ˆì—­í•˜ëŠ” ë° ì‚¬ìš©
- **re**: ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ë¥¼ í•„í„°ë§í•˜ëŠ” ë° ì‚¬ìš©

### ì‚¬ìš©ë²•

1. **ì˜ì¡´ì„± ì„¤ì¹˜:**
   ```bash
   pip install konlpy googletrans

2. **ì¹´í…Œê³ ë¦¬ í˜•íƒœì†Œ ë¶„ì„ ë° ë²ˆì—­ì„ ìˆ˜í–‰:**
   ```bash
   python category_corpus.py
ì¹´í…Œê³ ë¦¬ì—ì„œ ìƒˆë¡œìš´ ìœ ì‚¬ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ì—¬, output.json ì™€ output_oneElement.txt ì— ì €ìž¥

### íŒŒì´ì¬ ì½”ë“œ íŒŒì¼ (category_corpus.py)ì— ëŒ€í•œ ì„¤ëª…
- `tokenize_and_join(input_file: str) -> Tuple[List[int], List[str]]`: ìž…ë ¥ íŒŒì¼ì—ì„œ ê° ë¼ì¸ì„ ì½ì–´ì™€ í˜•íƒœì†Œ ë¶„ì„ ë° ë²ˆì—­ì„ ìˆ˜í–‰í•˜ì—¬ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ íŒŒì¼ë¡œ ì €ìž¥


 --- 
## Word2Vec í•œêµ­ì–´ ë‹¨ì–´ ìž„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
í•œêµ­ì–´ Word2Vec ìž„ë² ë”© ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë‹¨ì–´ ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ê³ , ì €ìž¥í•©ë‹ˆë‹¤.

### ì‚¬ìš©ëœ ì£¼ìš” ê¸°ìˆ  ë° ë¼ì´ë¸ŒëŸ¬ë¦¬

- **Word2Vec:** í•œêµ­ì–´ ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ë‹¨ì–´ ìž„ë² ë”© ëª¨ë¸ ê¸°ìˆ , Facebookì—ì„œ ì œê³µí•˜ëŠ” Word2Vec ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë‹¨ì–´ ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ê³  ì‚¬ìš©
- **SQLite:** ê²½ëŸ‰í™” DBMS ë¼ì´ë¸ŒëŸ¬ë¦¬, ë‹¨ì–´ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë¥¼ ì €ìž¥
- **unicodedata:** ìœ ë‹ˆì½”ë“œ ë¬¸ìžì— ëŒ€í•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
- **pickle:** `íŒŒì´ì¬ ê°ì²´ë¥¼ ì§ë ¬í™”í•˜ê³  ì—­ì§ë ¬í™”í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy:** ë‹¤ì°¨ì› ë°°ì—´ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

### ì‚¬ìš©ë²•

1. **ì˜ì¡´ì„± ì„¤ì¹˜:**
   ```bash
   pip install numpy tqdm

2. **í•œêµ­ì–´ Word2Vec ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•:**
   ```bash
   python process_vecs_*.py
í•œêµ­ì–´ Word2Vec ëª¨ë¸ì—ì„œ ë‹¨ì–´ ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ì—¬, *_guesses_ko.db ì™€ *_nearest_ko.dat ì— ì €ìž¥

### íŒŒì´ì¬ ì½”ë“œ íŒŒì¼ (process_vecs_*.py)ì— ëŒ€í•œ ì„¤ëª…
- `is_hangul(text) -> bool`: ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ê°€ í•œê¸€ì¸ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
- `load_dic(path: str) -> Set[str]`: ì£¼ì–´ì§„ ê²½ë¡œì—ì„œ ì‚¬ì „ íŒŒì¼ì„ ì½ì–´ì™€ ì§‘í•©(Set)ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜, ì‚¬ì „ì— í¬í•¨ëœ í•œê¸€ ë‹¨ì–´ë¥¼ ì •ê·œí™”í•˜ì—¬ ì €ìž¥
- `blocks(files, size=65536)`: íŒŒì¼ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜
- `count_lines(filepath)`: ì£¼ì–´ì§„ íŒŒì¼ì˜ ì´ ë¼ì¸ ìˆ˜ë¥¼ ì„¸ì–´ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
- ì£¼ì–´ì§„ Word2Vec ëª¨ë¸ì—ì„œ ë‹¨ì–´ ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ê³ , ë°ì´í„°ë² ì´ìŠ¤ì— ì €ìž¥
 

---
## ì˜ë¯¸ë¡ ì  ë‹¨ì–´ìœ ì‚¬ë„ë¥¼ í™œìš©í•œ ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ ì¶”ì²œ
ì €ìž¥ëœ ë‹¨ì–´ ë²¡í„°ë¥¼ í™œìš©í•˜ì—¬ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •, íŠ¹ì • ë‹¨ì–´ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ì°¾ê³ , í•´ë‹¹ ë‹¨ì–´ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ì‚¬ìš©ëœ ê¸°ìˆ  ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- **numpy:** ë‹¤ì°¨ì› ë°°ì—´ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **pickle:** íŒŒì´ì¬ ê°ì²´ë¥¼ ì§ë ¬í™”í•˜ê³  ì—­ì§ë ¬í™”í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
- **pymysql:** MySQL ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•˜ê³  ìƒí˜¸ìž‘ìš©í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

### ì‚¬ìš©ë²•

1. **ì˜ì¡´ì„± ì„¤ì¹˜:**
   ```bash
   pip install pymysql, numpy

2. **í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ ì¶”ì²œ, ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì²œ:**
   ```bash
   python process_smilar_*.py

 - `relCategory.json`: ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì¶”ì²œëœ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ìž¥
- `keyword/*.dat`: í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì²œëœ ê´€ë ¨ í‚¤ì›Œë“œ ì •ë³´ë¥¼ dat í˜•ì‹ìœ¼ë¡œ ì €ìž¥
- `category/*.json`: í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì²œëœ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ json í˜•ì‹ìœ¼ë¡œ ì €ìž¥
- 
### íŒŒì´ì¬ ì½”ë“œ íŒŒì¼ (process_smilar_*.py)ì— ëŒ€í•œ ì„¤ëª…

- `most_similar(mat: array, idx: int, k: int) -> Tuple[array, array]`: íŠ¹ì • ë‹¨ì–´ì— ëŒ€í•´ ì£¼ì–´ì§„ í–‰ë ¬ì—ì„œ ê°€ìž¥ ìœ ì‚¬í•œ kê°œì˜ ë‹¨ì–´ì™€ ê·¸ ìœ ì‚¬ë„ë¥¼ ë°˜í™˜
- `dump_nearest(title: str, values: List[str], words: List[str], mat: array, k: int = 100) -> List[str]`: ë‹¨ì–´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ íŒŒì¼ë¡œ ì €ìž¥, ì´ë¯¸ ê³„ì‚°ëœ ê²°ê³¼ê°€ ìžˆëŠ” ê²½ìš° íŒŒì¼ì—ì„œ ë¡œë“œí•˜ì—¬ ë°˜í™˜
- `get_nearest(title: str, values: List[str], words: List[str], mat: array) -> List[str]`: ë‹¨ì–´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¯¸ ê³„ì‚°ëœ ê²°ê³¼ê°€ ìžˆëŠ”ì§€ í™•ì¸í•œ í›„ ìžˆìœ¼ë©´ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•˜ê³ , ì—†ìœ¼ë©´ ë‹¤ì‹œ ê³„ì‚°í•˜ì—¬ ë°˜í™˜


---
## ë¼ì´ì„ ìŠ¤
ì´ í”„ë¡œì íŠ¸ëŠ” GPL-3.0 ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¥´ë©°, ìžì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.


---

## View English Introduction

## _Index 
- [Directory](#_Directory)
- [DataSet](#_DataSet)
- [Optimizing Vector Similarity using the Annoy Library](#Optimizing-Vector-Similarity-using-the-Annoy-Library)
- [Visualizing Vector Data with t-SNE](#Visualizing-Vector-Data-with-t-SNE)
- [Word Filtering using BERT for Korean Text Classification](#Word-Filtering-using-BERT-for-Korean-Text-Classification)
- [Category Morphological Analysis and Translation using konlpy and googletrans](#Category-Morphological-Analysis-and-Translation-using-konlpy-and-googletrans)
- [Building a Word2Vec Korean Word Embedding Database](#Building-a-Word2Vec-Korean-Word-Embedding-Database)
- [Category/Keyword Recommendation using Semantic Word Similarity](#categorykeyword-recommendation-using-semantic-word-similarity)
- [License](#_License)

---
## _Directory
- [`USER_CTGY`](USER_CTGY): User-based Collaborative Filtering for Category Recommendation
- [`USER_MODL`](USER_MODL): Item-based Collaborative Filtering for Module-specific Feature Recommendation
- [`SMLR_RECO`](SMLR_RECO): Semantic Keyword and Category Recommendation Algorithm using Morphological Analysis and Tagging Libraries
  1. [`ByCTGY`](SMLR_RECO/ByCTGY): Category-based Related Category Recommendation Algorithm
  2. [`ByKYWD`](SMLR_RECO/ByKYWD): Keyword-based Related Category and Keyword Recommendation Algorithm
  

---
## _DataSet
1. Analyzing explicit user data and implicit feedback through category/module behavior records to create category/module vector data
   - Dynamic Vector Weights: Points used, more requests, searches, stays longer than 30 seconds, activate/save updates, likes/comments
   - Static Vector Weights: Module, category, keyword, annual keyword
2. [Korean Basic Dictionary](https://krdict.korean.go.kr/), [Standard Korean Dictionary](https://stdict.korean.go.kr/), [Woori-mal-saem](https://opendict.korean.go.kr/) based [Korean Spelling Dictionary](https://github.com/spellcheck-ko/hunspell-dict-ko/releases/download/0.7.92/ko-aff-dic-0.7.92.zip) provided by spellcheck-ko
3. Korean Word2Vec Model [Korean Word Vectors](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ko.300.vec.gz) represented in 300 dimensions provided by Facebook
4. Naver categories divided and analyzed by morphological analysis

---
## Optimizing Vector Similarity using the Annoy Library
Optimizing vector similarity using the [`Annoy`](https://github.com/spotify/annoy) library and [`Bayesian Optimization`](https://github.com/bayesian-optimization/BayesianOptimization).

### Key Technologies and Libraries Used
- **Annoy Library:** Efficient library for calculating and searching vector similarity
- **Bayesian Optimization:** Efficient algorithm for optimizing objective functions
- **pandas:** Library for data manipulation and calculation
- **numpy:** Library for handling multi-dimensional arrays
- **matplotlib:** Library for data visualization

### Usage
1. **Install Dependencies:**
   ```bash
   pip install annoy pandas numpy scikit-learn bayesian-optimization matplotlib

2. **Run the Code:**
   ```bash
   python *_optimizeAnnModel.py
Run the above command to perform vector similarity optimization using the Annoy library.

### Explanation of Python Code File (*_similarity_optimization.py)
- `evaluate_n_trees(n_trees)`: Function to optimize the accuracy of the Annoy index, calculates vector similarity for a given number of trees, and returns the average distance of the nearest neighbors
- `BayesianOptimization`: Initializes and configures the text classification pipeline, uses the BERT model to perform classification on the input text, and returns the results

---  
## Visualizing Vector Data with t-SNE 
Using scikit-learn's [`t-SNE`](https://github.com/scikit-learn/scikit-learn/tree/main) algorithm to visualize vector data.

### Key Technologies and Libraries Used
- **t-SNE:** Algorithm used to visualize high-dimensional data by reducing it to lower dimensions while preserving the structure
- **matplotlib:** Data visualization library
- **scikit-learn:** Library for implementing machine learning models
- **pandas:** Library for data manipulation and calculation
- **numpy:** Library for handling multi-dimensional arrays

### Usage

1. **Install Dependencies:**
   ```bash
   pip install scikit-learn matplotlib pandas numpy
2. **Run the Code:**
   ```bash 
   python visualize_vectors.py
Results in 2D and 3D t-SNE visualizations are generated as images named TSNE_2D.png and TSNE_3D.png.

--- 
## Word Filtering using BERT for Korean Text Classification
Using the BERT pre-trained language model for natural language processing (NLP) tasks to filter clean words from text.

### Key Technologies and Libraries Used
- **BERT:** Pre-trained language model based on the bidirectional transformer model, using the [`kor_unsmile`](https://github.com/smilegate-ai/korean_unsmile_dataset) model provided by Smilegate-ai
- **Hugging Face Transformers:** Library providing various pre-trained models for different languages, loads the model and performs text classification using BERT

### Usage

1. **Install Dependencies:**
   ```bash
   pip install transformers tqdm

2. **Download Pre-trained BERT Model and Tokenizer:**
    ```python
    from transformers import BertForSequenceClassification, AutoTokenizer
    
    model_name = 'smilegate-ai/kor_unsmile'
    model = BertForSequenceClassification.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
3. **Perform Word Filtering:**
    ```bash
   python filter_words.py
Categorizes Korean words provided by spellcheck-ko, extracts clean words, and saves the results in the data/ko_filtered.txt file.

### Explanation of Python Code File (filter_words.py)
 - `get_predicated_label(output_labels, min_score)`: Function to return only labels from the BERT model's output that have a score greater than or equal to the specified minimum score

- `TextClassificationPipeline`: Initializes and configures the text classification pipeline, uses the BERT model to perform classification on the input text, and returns the results

---
## Category Morphological Analysis and Translation using konlpy and googletrans 
Utilizing various tagging libraries from [`KoNLPy`](https://github.com/konlpy/konlpy) for Korean morphological analysis to extract meaningful words from categories. Translates extracted words using [`googletrans`](https://github.com/ssut/py-googletrans), then normalizes them to obtain new similar words.

### Key Technologies and Libraries Used
- **konlpy:** Library for Korean morphological analysis, using Okt, Hannanum, Kkma, and Komoran for morphological analysis
- **googletrans:** Library using the Google Translate API for word translation
- **re:** Library for regular expressions used to filter words

### Usage

1. **Install Dependencies:**
   ```bash
   pip install konlpy googletrans

2. **Perform Category Morphological Analysis and Translation:**
   ```bash
   python category_corpus.py
Extracts new similar words from categories, saves the results in output.json and output_oneElement.txt.

### Explanation of Python Code File (category_corpus.py)
- `tokenize_and_join(input_file: str) -> Tuple[List[int], List[str]]`: Reads each line from the input file, performs morphological analysis and translation to extract meaningful words, and saves them to a file


 --- 
## Building a Word2Vec Korean Word Embedding Database
Extracting word vectors using the Korean Word2Vec embedding model and saving them.

### Key Technologies and Libraries Used

- **Word2Vec:** Technique for learning distributed representations of words, using Facebook's Word2Vec model to extract and use word vectors
- **SQLite:** Lightweight database management system, used to store words and their corresponding vectors
- **unicodedata:** Library providing a database for Unicode characters
- **pickle:** Library for serializing and deserializing Python objects
- **numpy:** Library for handling multi-dimensional arrays

### Usage

1. **Install Dependencies:**
   ```bash
   pip install numpy tqdm

2. **Build Korean Word2Vec Database:**
   ```bash
   python process_vecs_*.py
Extracts word vectors from the Korean Word2Vec model and saves them in *_guesses_ko.db and *_nearest_ko.dat.

### Explanation of Python Code File (process_vecs_*.py)
- `is_hangul(text) -> bool`: Function to check if the given text is in Hangul (Korean)
- `load_dic(path: str) -> Set[str]`: Function to read the dictionary file from the specified path and return it as a set, normalizing Korean words included in the dictionary
- `blocks(files, size=65536)`: Generator function to divide a file into blocks
- `count_lines(filepath)`: Function to count the total number of lines in a given file
- Extracts word vectors from the Word2Vec model and stores them in the database

---
## Category/Keyword Recommendation using Semantic Word Similarity
Using stored word vectors to measure similarity between words, find similar words for a specific word, and recommend categories based on those words.

### Key Technologies and Libraries Used
- **numpy:** Library for handling multi-dimensional arrays
- **pickle:** Library for serializing and deserializing Python objects
- **pymysql:** Library for connecting to and interacting with MySQL databases

### Usage

1. **Install Dependencies:**
   ```bash
   pip install pymysql, numpy

2. **Perform Keyword-based Category/Keyword Recommendation, Category-based Category Recommendation:**
   ```bash
   python process_smilar_*.py
- `relCategory.json`: JSON file storing information about recommended related categories based on category recommendations
- `keyword/*.dat`: dat files storing information about related keywords recommended based on keyword recommendations
- `category/*.json`: JSON files storing information about related categories recommended based on keyword recommendations

### Explanation of Python Code File (process_smilar_*.py) - Continued

- `get_word_vector(word: str, model: Word2Vec) -> Optional[array]`: Function to retrieve the vector representation of a given word from the Word2Vec model
- `recommend_by_category(category: str, k: int = 5) -> List[str]`: Recommends related categories based on the semantic similarity of words within the given category
- `recommend_by_keyword(keyword: str, k: int = 5) -> List[str]`: Recommends related keywords based on the semantic similarity of words within the given keyword
- `dump_json(data: Any, filepath: str)`: Serializes the given data to a JSON file
- `load_json(filepath: str) -> Any`: Deserializes the data from a JSON file

---

## _License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
